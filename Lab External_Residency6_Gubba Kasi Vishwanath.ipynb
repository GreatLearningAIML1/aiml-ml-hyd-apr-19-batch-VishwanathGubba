{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYk8NG3yOIT9"
   },
   "source": [
    "### A MNIST-like fashion product database\n",
    "\n",
    "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFO6PuxzOIT_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efNjNImfOIUC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9C4aAIGOIUH",
    "outputId": "5ef9aff6-a7bd-4b26-cba6-8750955f6ca3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcoZBStrOIUQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnbx7TyQOIUY"
   },
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbiHj5YPOIUc",
    "outputId": "87e1b9cd-07f0-45cb-e706-0d51ad742d72",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6]\n"
     ]
    }
   ],
   "source": [
    "print(testY[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDAYzkwyOIUj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convert both training and testing labels into one-hot vectors.\n",
    "\n",
    "**Hint:** check **tf.keras.utils.to_categorical()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBlfYlANOIUk"
   },
   "outputs": [],
   "source": [
    "trainY2 = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
    "testY2 = tf.keras.utils.to_categorical(testY, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHV3b9mzOIUq",
    "outputId": "27bdfe58-91ee-4677-fe49-e742ad306c70",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "First 5 examples now are:  [9 0 0 3 0]\n"
     ]
    }
   ],
   "source": [
    "print(trainY2.shape)\n",
    "print('First 5 examples now are: ', trainY[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhQ8e7VOIUw"
   },
   "source": [
    "### Visualize the data\n",
    "\n",
    "Plot first 10 images in the triaining set and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvDML2OoOIUx",
    "outputId": "9dafc94e-61a8-4089-be03-d143163d68aa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(trainX[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel([trainY[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4TbJGeSOIU4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac06XZZTOIU6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0928 15:46:19.836203  6720 deprecation.py:506] From E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hQpLv3aOIU_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model using model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O59C_-IgOIVB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 2041.5273 - acc: 0.7384 - val_loss: 1337.2847 - val_acc: 0.7747\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1654.4849 - acc: 0.7778 - val_loss: 1638.0034 - val_acc: 0.7467\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 1553.7005 - acc: 0.7844 - val_loss: 2491.6462 - val_acc: 0.7440\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 1499.2094 - acc: 0.7887 - val_loss: 1117.2693 - val_acc: 0.8065\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1475.3057 - acc: 0.7939 - val_loss: 1657.5610 - val_acc: 0.7909\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1510.7721 - acc: 0.7958 - val_loss: 1135.0295 - val_acc: 0.8117\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 1438.3112 - acc: 0.7956 - val_loss: 2429.7069 - val_acc: 0.7683\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1435.2848 - acc: 0.7969 - val_loss: 1402.5063 - val_acc: 0.7750\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1466.1531 - acc: 0.7988 - val_loss: 1246.0192 - val_acc: 0.7966\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 1428.3807 - acc: 0.7995 - val_loss: 2074.3047 - val_acc: 0.7439\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 1440.8311 - acc: 0.8008 - val_loss: 1236.5070 - val_acc: 0.8070\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 1399.4881 - acc: 0.8012 - val_loss: 1313.8494 - val_acc: 0.7840\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 1400.1372 - acc: 0.8035 - val_loss: 1089.7633 - val_acc: 0.7998\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 1404.6704 - acc: 0.8023 - val_loss: 2148.0160 - val_acc: 0.7132\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 1416.9449 - acc: 0.8034 - val_loss: 1061.6883 - val_acc: 0.7921\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1415.3431 - acc: 0.8022 - val_loss: 1530.8750 - val_acc: 0.7832\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 1375.7548 - acc: 0.8050 - val_loss: 1584.1372 - val_acc: 0.7669\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 1381.8502 - acc: 0.8036 - val_loss: 1050.0994 - val_acc: 0.8185\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 1378.2070 - acc: 0.8037 - val_loss: 1183.1715 - val_acc: 0.8022\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 1416.6224 - acc: 0.8051 - val_loss: 3235.1265 - val_acc: 0.7591\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 1406.4215 - acc: 0.8052 - val_loss: 1369.9883 - val_acc: 0.7913\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 1390.4937 - acc: 0.8047 - val_loss: 1338.9588 - val_acc: 0.8191\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 1372.8212 - acc: 0.8073 - val_loss: 1735.8816 - val_acc: 0.7465\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 1380.3686 - acc: 0.8060 - val_loss: 994.4406 - val_acc: 0.8289\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 1424.4862 - acc: 0.8054 - val_loss: 1569.3894 - val_acc: 0.7740\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 1449.8776 - acc: 0.8049 - val_loss: 1056.8075 - val_acc: 0.8169\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 1327.6908 - acc: 0.8087 - val_loss: 1362.9948 - val_acc: 0.8079\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 1384.8748 - acc: 0.8073 - val_loss: 2206.5541 - val_acc: 0.7833\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 1370.4480 - acc: 0.8096 - val_loss: 1164.2930 - val_acc: 0.8096\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 1377.4188 - acc: 0.8087 - val_loss: 1278.5668 - val_acc: 0.8142\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1376.0415 - acc: 0.8081 - val_loss: 2389.4929 - val_acc: 0.7593\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 1341.8704 - acc: 0.8081 - val_loss: 1358.8567 - val_acc: 0.7742\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 1367.2972 - acc: 0.8082 - val_loss: 1098.4902 - val_acc: 0.8129\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 1398.5176 - acc: 0.8075 - val_loss: 1488.0850 - val_acc: 0.7827\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 1389.5322 - acc: 0.8090 - val_loss: 1391.1811 - val_acc: 0.7890\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 1348.4808 - acc: 0.8114 - val_loss: 1884.2553 - val_acc: 0.7718\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1346.1793 - acc: 0.8090 - val_loss: 1531.3138 - val_acc: 0.7963\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 1369.0357 - acc: 0.8094 - val_loss: 1828.9893 - val_acc: 0.8003\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 1353.2227 - acc: 0.8099 - val_loss: 3314.2398 - val_acc: 0.7245\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 1339.5195 - acc: 0.8109 - val_loss: 1127.2560 - val_acc: 0.8078\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 1364.9008 - acc: 0.8097 - val_loss: 1007.4921 - val_acc: 0.8203\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1368.8679 - acc: 0.8108 - val_loss: 1538.5550 - val_acc: 0.7694\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 1361.9793 - acc: 0.8111 - val_loss: 1625.0571 - val_acc: 0.7987\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 1367.0786 - acc: 0.8107 - val_loss: 1428.2323 - val_acc: 0.7778\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1338.1147 - acc: 0.8114 - val_loss: 1277.9121 - val_acc: 0.7778\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 1396.9915 - acc: 0.8087 - val_loss: 3506.4026 - val_acc: 0.7270\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1382.6608 - acc: 0.8083 - val_loss: 1956.1131 - val_acc: 0.7773\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1358.3650 - acc: 0.8122 - val_loss: 1309.7234 - val_acc: 0.8028\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1366.7464 - acc: 0.8099 - val_loss: 1190.1400 - val_acc: 0.8136\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 1365.5420 - acc: 0.8123 - val_loss: 1225.5105 - val_acc: 0.8073\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 1322.0932 - acc: 0.8119 - val_loss: 2106.2958 - val_acc: 0.7405\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 1351.5191 - acc: 0.8083 - val_loss: 1740.1953 - val_acc: 0.7830\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 1335.2121 - acc: 0.8117 - val_loss: 1396.8249 - val_acc: 0.7967\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 1350.4335 - acc: 0.8113 - val_loss: 1049.9231 - val_acc: 0.8233\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 1392.8050 - acc: 0.8123 - val_loss: 1684.4555 - val_acc: 0.7944\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 3s 43us/sample - loss: 1314.3611 - acc: 0.8132 - val_loss: 1085.8904 - val_acc: 0.8136\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1379.9359 - acc: 0.8102 - val_loss: 1086.5778 - val_acc: 0.8233\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1307.2233 - acc: 0.8129 - val_loss: 1061.7485 - val_acc: 0.8183\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1354.7005 - acc: 0.8105 - val_loss: 1227.7240 - val_acc: 0.7950\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1368.4282 - acc: 0.8105 - val_loss: 1342.6873 - val_acc: 0.8110\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1381.2901 - acc: 0.8102 - val_loss: 2053.9535 - val_acc: 0.7448\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1346.7332 - acc: 0.8126 - val_loss: 2193.1869 - val_acc: 0.7230\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1343.9131 - acc: 0.8115 - val_loss: 3020.8507 - val_acc: 0.7667\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1334.1552 - acc: 0.8132 - val_loss: 1124.0027 - val_acc: 0.8196\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1338.8165 - acc: 0.8133 - val_loss: 1825.1179 - val_acc: 0.7737\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 1374.9312 - acc: 0.8091 - val_loss: 1278.2652 - val_acc: 0.8113\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 1359.1754 - acc: 0.8119 - val_loss: 1988.9243 - val_acc: 0.7893\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 1359.4021 - acc: 0.8103 - val_loss: 1580.9424 - val_acc: 0.7705\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 1320.8518 - acc: 0.8142 - val_loss: 1457.5297 - val_acc: 0.7813\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 1301.1000 - acc: 0.8147 - val_loss: 1379.1163 - val_acc: 0.7861\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 1327.4904 - acc: 0.8117 - val_loss: 2095.0046 - val_acc: 0.7671\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1364.1717 - acc: 0.8121 - val_loss: 1733.5712 - val_acc: 0.7620\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1343.6153 - acc: 0.8128 - val_loss: 1459.0536 - val_acc: 0.8023\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 1294.6035 - acc: 0.8152 - val_loss: 1362.6488 - val_acc: 0.7907\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 1334.4833 - acc: 0.8126 - val_loss: 1566.1731 - val_acc: 0.7664\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 1310.3079 - acc: 0.8120 - val_loss: 1500.9613 - val_acc: 0.7829\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1374.6945 - acc: 0.8105 - val_loss: 1524.2126 - val_acc: 0.7876\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 1329.6166 - acc: 0.8140 - val_loss: 1549.8596 - val_acc: 0.7831\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1367.6592 - acc: 0.8116 - val_loss: 2388.3330 - val_acc: 0.7202\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1332.7280 - acc: 0.8124 - val_loss: 1582.5017 - val_acc: 0.7763\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1328.3709 - acc: 0.8144 - val_loss: 1209.7983 - val_acc: 0.8159\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1320.7602 - acc: 0.8120 - val_loss: 1220.0443 - val_acc: 0.7974\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1325.6176 - acc: 0.8130 - val_loss: 2641.9115 - val_acc: 0.7767\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1334.2902 - acc: 0.8121 - val_loss: 1310.6763 - val_acc: 0.7955\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1358.4621 - acc: 0.8119 - val_loss: 1257.6027 - val_acc: 0.7964\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1342.1507 - acc: 0.8130 - val_loss: 1403.8425 - val_acc: 0.8120\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1318.8215 - acc: 0.8136 - val_loss: 1292.3697 - val_acc: 0.7892\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1344.1133 - acc: 0.8135 - val_loss: 1136.4044 - val_acc: 0.8220\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1326.6207 - acc: 0.8127 - val_loss: 1292.9042 - val_acc: 0.8129\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1339.3352 - acc: 0.8136 - val_loss: 1506.7500 - val_acc: 0.7923\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 1319.4223 - acc: 0.8144 - val_loss: 2524.6856 - val_acc: 0.7514\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1339.9016 - acc: 0.8137 - val_loss: 3401.2729 - val_acc: 0.6674\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1323.3635 - acc: 0.8139 - val_loss: 1466.7945 - val_acc: 0.7720\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1349.2903 - acc: 0.8142 - val_loss: 1634.7482 - val_acc: 0.7633\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1294.1982 - acc: 0.8143 - val_loss: 1424.8396 - val_acc: 0.8122\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1318.1566 - acc: 0.8134 - val_loss: 3284.5798 - val_acc: 0.7443\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1391.3002 - acc: 0.8106 - val_loss: 1437.5744 - val_acc: 0.7980\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1324.3763 - acc: 0.8127 - val_loss: 3065.6735 - val_acc: 0.7117\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1327.4792 - acc: 0.8147 - val_loss: 1074.8289 - val_acc: 0.8231\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 1318.5228 - acc: 0.8134 - val_loss: 2463.6888 - val_acc: 0.7901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7cf4c63b70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY2, \n",
    "          validation_data=(testX, testY2), \n",
    "          epochs=100,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdzDtGwDOIVF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kndfpdidOIVI"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwk3T5LJOIVN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNLR8tcBOIVP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.5904 - acc: 0.7981 - val_loss: 0.5271 - val_acc: 0.8218\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4899 - acc: 0.8315 - val_loss: 0.4832 - val_acc: 0.8323\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4650 - acc: 0.8399 - val_loss: 0.4690 - val_acc: 0.8385\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4570 - acc: 0.8429 - val_loss: 0.4763 - val_acc: 0.8326\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4500 - acc: 0.8452 - val_loss: 0.4587 - val_acc: 0.8426\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4431 - acc: 0.8453 - val_loss: 0.4647 - val_acc: 0.8404\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4389 - acc: 0.8484 - val_loss: 0.4762 - val_acc: 0.8366\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4352 - acc: 0.8500 - val_loss: 0.4847 - val_acc: 0.8397\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4342 - acc: 0.8484 - val_loss: 0.4618 - val_acc: 0.8402\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4305 - acc: 0.8499 - val_loss: 0.4579 - val_acc: 0.8425\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4288 - acc: 0.8514 - val_loss: 0.4658 - val_acc: 0.8402\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4276 - acc: 0.8532 - val_loss: 0.4630 - val_acc: 0.8407\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4244 - acc: 0.8511 - val_loss: 0.4584 - val_acc: 0.8402\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4248 - acc: 0.8515 - val_loss: 0.4895 - val_acc: 0.8412\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4213 - acc: 0.8537 - val_loss: 0.4682 - val_acc: 0.8417\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.4207 - acc: 0.8544 - val_loss: 0.4761 - val_acc: 0.8421\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4184 - acc: 0.8544 - val_loss: 0.4646 - val_acc: 0.8411\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4189 - acc: 0.8534 - val_loss: 0.4680 - val_acc: 0.8405\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4173 - acc: 0.8526 - val_loss: 0.4592 - val_acc: 0.8417\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4159 - acc: 0.8546 - val_loss: 0.4609 - val_acc: 0.8415\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4176 - acc: 0.8552 - val_loss: 0.4670 - val_acc: 0.8398\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4159 - acc: 0.8549 - val_loss: 0.4634 - val_acc: 0.8393\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4143 - acc: 0.8541 - val_loss: 0.4679 - val_acc: 0.8407\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.4130 - acc: 0.8545 - val_loss: 0.4807 - val_acc: 0.8376\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4151 - acc: 0.8544 - val_loss: 0.4641 - val_acc: 0.8408\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4139 - acc: 0.8558 - val_loss: 0.4710 - val_acc: 0.8438\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4128 - acc: 0.8544 - val_loss: 0.4691 - val_acc: 0.8398\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4099 - acc: 0.8555 - val_loss: 0.4702 - val_acc: 0.8400\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4113 - acc: 0.8555 - val_loss: 0.4597 - val_acc: 0.8415\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4102 - acc: 0.8556 - val_loss: 0.4587 - val_acc: 0.8435\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4097 - acc: 0.8563 - val_loss: 0.4666 - val_acc: 0.8395\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4100 - acc: 0.8551 - val_loss: 0.4598 - val_acc: 0.8439\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4098 - acc: 0.8556 - val_loss: 0.4750 - val_acc: 0.8432\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4084 - acc: 0.8569 - val_loss: 0.4678 - val_acc: 0.8415\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4088 - acc: 0.8548 - val_loss: 0.4689 - val_acc: 0.8392\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4079 - acc: 0.8548 - val_loss: 0.4810 - val_acc: 0.8438\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4057 - acc: 0.8578 - val_loss: 0.4732 - val_acc: 0.8415\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4073 - acc: 0.8564 - val_loss: 0.4667 - val_acc: 0.8415\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4068 - acc: 0.8574 - val_loss: 0.4779 - val_acc: 0.8429\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4088 - acc: 0.8563 - val_loss: 0.4664 - val_acc: 0.8425\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4059 - acc: 0.8568 - val_loss: 0.4969 - val_acc: 0.8440\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4100 - acc: 0.8557 - val_loss: 0.4987 - val_acc: 0.8395\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4071 - acc: 0.8553 - val_loss: 0.5012 - val_acc: 0.8419\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4067 - acc: 0.8577 - val_loss: 0.4725 - val_acc: 0.8412\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4057 - acc: 0.8569 - val_loss: 0.4673 - val_acc: 0.8397\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4069 - acc: 0.8553 - val_loss: 0.4732 - val_acc: 0.8434\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4029 - acc: 0.8591 - val_loss: 0.4629 - val_acc: 0.8387\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4035 - acc: 0.8582 - val_loss: 0.4719 - val_acc: 0.8435\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.4040 - acc: 0.8582 - val_loss: 0.4752 - val_acc: 0.8416\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4052 - acc: 0.8578 - val_loss: 0.4958 - val_acc: 0.8396\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4055 - acc: 0.8570 - val_loss: 0.4686 - val_acc: 0.8438\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4073 - acc: 0.8569 - val_loss: 0.4789 - val_acc: 0.8393\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4055 - acc: 0.8561 - val_loss: 0.4842 - val_acc: 0.8436\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4036 - acc: 0.8563 - val_loss: 0.4661 - val_acc: 0.8413\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4048 - acc: 0.8564 - val_loss: 0.4937 - val_acc: 0.8415\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4042 - acc: 0.8574 - val_loss: 0.4769 - val_acc: 0.8418\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 10s 168us/sample - loss: 0.4045 - acc: 0.8576 - val_loss: 0.5212 - val_acc: 0.8404\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.4032 - acc: 0.8560 - val_loss: 0.4917 - val_acc: 0.8401\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4021 - acc: 0.8583 - val_loss: 0.4750 - val_acc: 0.8419\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4031 - acc: 0.8585 - val_loss: 0.4898 - val_acc: 0.8415\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4009 - acc: 0.8582 - val_loss: 0.4677 - val_acc: 0.8403\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4033 - acc: 0.8576 - val_loss: 0.4811 - val_acc: 0.8428\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4022 - acc: 0.8589 - val_loss: 0.4675 - val_acc: 0.8438\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4037 - acc: 0.8575 - val_loss: 0.4976 - val_acc: 0.8410\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4030 - acc: 0.8581 - val_loss: 0.5087 - val_acc: 0.8411\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4003 - acc: 0.8578 - val_loss: 0.4821 - val_acc: 0.8431\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4003 - acc: 0.8579 - val_loss: 0.4830 - val_acc: 0.8424\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4014 - acc: 0.8585 - val_loss: 0.5235 - val_acc: 0.8401\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4026 - acc: 0.8597 - val_loss: 0.4955 - val_acc: 0.8412\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4010 - acc: 0.8582 - val_loss: 0.4840 - val_acc: 0.8388\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.4010 - acc: 0.8581 - val_loss: 0.4837 - val_acc: 0.8401\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4014 - acc: 0.8591 - val_loss: 0.4986 - val_acc: 0.8430\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.4020 - acc: 0.8586 - val_loss: 0.5174 - val_acc: 0.8380\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4029 - acc: 0.8585 - val_loss: 0.4877 - val_acc: 0.8379\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4032 - acc: 0.8575 - val_loss: 0.4760 - val_acc: 0.8408\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.4048 - acc: 0.8567 - val_loss: 0.4738 - val_acc: 0.8416\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4029 - acc: 0.8579 - val_loss: 0.4951 - val_acc: 0.8402\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4003 - acc: 0.8585 - val_loss: 0.4691 - val_acc: 0.8404\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.3988 - acc: 0.8604 - val_loss: 0.4758 - val_acc: 0.8401\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4021 - acc: 0.8577 - val_loss: 0.4750 - val_acc: 0.8421\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4026 - acc: 0.8579 - val_loss: 0.4881 - val_acc: 0.8382\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4000 - acc: 0.8584 - val_loss: 0.4786 - val_acc: 0.8388\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.4001 - acc: 0.8586 - val_loss: 0.4868 - val_acc: 0.8432\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4010 - acc: 0.8572 - val_loss: 0.4995 - val_acc: 0.8385\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.4005 - acc: 0.8587 - val_loss: 0.4988 - val_acc: 0.8421\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4017 - acc: 0.8579 - val_loss: 0.4869 - val_acc: 0.8401\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4009 - acc: 0.8581 - val_loss: 0.4783 - val_acc: 0.8402\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.4001 - acc: 0.8586 - val_loss: 0.4685 - val_acc: 0.8404\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 0.4001 - acc: 0.8593 - val_loss: 0.4818 - val_acc: 0.8416\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.4021 - acc: 0.8575 - val_loss: 0.4901 - val_acc: 0.8374\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4007 - acc: 0.8579 - val_loss: 0.4947 - val_acc: 0.8417\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3998 - acc: 0.8576 - val_loss: 0.4799 - val_acc: 0.8394\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3971 - acc: 0.8614 - val_loss: 0.4827 - val_acc: 0.8390\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 0.3999 - acc: 0.8583 - val_loss: 0.4957 - val_acc: 0.8372\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3996 - acc: 0.8572 - val_loss: 0.4859 - val_acc: 0.8403\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3978 - acc: 0.8599 - val_loss: 0.4901 - val_acc: 0.8417\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4005 - acc: 0.8586 - val_loss: 0.4950 - val_acc: 0.8391\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4000 - acc: 0.8587 - val_loss: 0.5035 - val_acc: 0.8355\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4008 - acc: 0.8576 - val_loss: 0.4834 - val_acc: 0.8415\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3991 - acc: 0.8585 - val_loss: 0.4956 - val_acc: 0.8403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7cf4b13080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY2, \n",
    "          validation_data=(testX, testY2), \n",
    "          epochs=100,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Py-KwkmjOIVU"
   },
   "source": [
    "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLXUE9jWOIVV"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "sgd = optimizers.SGD(lr=0.001)\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJUqA5T4OIVc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.6022 - acc: 0.7927 - val_loss: 0.5165 - val_acc: 0.8215\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4911 - acc: 0.8304 - val_loss: 0.4944 - val_acc: 0.8313\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4689 - acc: 0.8387 - val_loss: 0.4965 - val_acc: 0.8322\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4566 - acc: 0.8426 - val_loss: 0.4818 - val_acc: 0.8348\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4491 - acc: 0.8451 - val_loss: 0.4682 - val_acc: 0.8347\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4420 - acc: 0.8466 - val_loss: 0.4679 - val_acc: 0.8369\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.4404 - acc: 0.8475 - val_loss: 0.4644 - val_acc: 0.8380\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.4387 - acc: 0.8476 - val_loss: 0.4694 - val_acc: 0.8374\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4335 - acc: 0.8497 - val_loss: 0.4694 - val_acc: 0.8388\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4295 - acc: 0.8494 - val_loss: 0.4666 - val_acc: 0.8409\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4289 - acc: 0.8508 - val_loss: 0.4674 - val_acc: 0.8388\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4246 - acc: 0.8534 - val_loss: 0.4629 - val_acc: 0.8403\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4255 - acc: 0.8518 - val_loss: 0.4687 - val_acc: 0.8396\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4208 - acc: 0.8540 - val_loss: 0.4680 - val_acc: 0.8419\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4240 - acc: 0.8527 - val_loss: 0.4628 - val_acc: 0.8427\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4222 - acc: 0.8533 - val_loss: 0.4667 - val_acc: 0.8405\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4198 - acc: 0.8539 - val_loss: 0.4694 - val_acc: 0.8401\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4179 - acc: 0.8539 - val_loss: 0.4731 - val_acc: 0.8406\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4169 - acc: 0.8551 - val_loss: 0.4681 - val_acc: 0.8406\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4168 - acc: 0.8532 - val_loss: 0.4728 - val_acc: 0.8416\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.4163 - acc: 0.8541 - val_loss: 0.4834 - val_acc: 0.8400\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4144 - acc: 0.8559 - val_loss: 0.4920 - val_acc: 0.8362\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4135 - acc: 0.8550 - val_loss: 0.4604 - val_acc: 0.8412\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4132 - acc: 0.8565 - val_loss: 0.4717 - val_acc: 0.8431\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4135 - acc: 0.8549 - val_loss: 0.4717 - val_acc: 0.8421\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4113 - acc: 0.8567 - val_loss: 0.4748 - val_acc: 0.8420\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4141 - acc: 0.8560 - val_loss: 0.4645 - val_acc: 0.8418\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4120 - acc: 0.8548 - val_loss: 0.4660 - val_acc: 0.8405\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.4117 - acc: 0.8551 - val_loss: 0.4736 - val_acc: 0.8404\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4094 - acc: 0.8560 - val_loss: 0.4765 - val_acc: 0.8434\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4104 - acc: 0.8559 - val_loss: 0.4846 - val_acc: 0.8398\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4079 - acc: 0.8565 - val_loss: 0.4698 - val_acc: 0.8419\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4117 - acc: 0.8550 - val_loss: 0.4694 - val_acc: 0.8411\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4099 - acc: 0.8565 - val_loss: 0.4700 - val_acc: 0.8401\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4071 - acc: 0.8569 - val_loss: 0.4790 - val_acc: 0.8409\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.4087 - acc: 0.8578 - val_loss: 0.4943 - val_acc: 0.8365\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4072 - acc: 0.8566 - val_loss: 0.4700 - val_acc: 0.8422\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4100 - acc: 0.8554 - val_loss: 0.4768 - val_acc: 0.8392\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4087 - acc: 0.8567 - val_loss: 0.4647 - val_acc: 0.8433\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4064 - acc: 0.8564 - val_loss: 0.4826 - val_acc: 0.8367\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4077 - acc: 0.8569 - val_loss: 0.4840 - val_acc: 0.8389\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4069 - acc: 0.8573 - val_loss: 0.4775 - val_acc: 0.8404\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4074 - acc: 0.8559 - val_loss: 0.4772 - val_acc: 0.8420\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4076 - acc: 0.8570 - val_loss: 0.4831 - val_acc: 0.8428\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4094 - acc: 0.8561 - val_loss: 0.4787 - val_acc: 0.8381\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.4066 - acc: 0.8567 - val_loss: 0.4733 - val_acc: 0.8404\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4054 - acc: 0.8573 - val_loss: 0.4942 - val_acc: 0.8418\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4051 - acc: 0.8569 - val_loss: 0.5123 - val_acc: 0.8407\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.4046 - acc: 0.8575 - val_loss: 0.4688 - val_acc: 0.8387\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4054 - acc: 0.8577 - val_loss: 0.4663 - val_acc: 0.8398\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4065 - acc: 0.8565 - val_loss: 0.4861 - val_acc: 0.8411\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.4048 - acc: 0.8569 - val_loss: 0.4797 - val_acc: 0.8402\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4031 - acc: 0.8585 - val_loss: 0.4646 - val_acc: 0.8416\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.4055 - acc: 0.8564 - val_loss: 0.4847 - val_acc: 0.8416\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4042 - acc: 0.8578 - val_loss: 0.4839 - val_acc: 0.8416\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4030 - acc: 0.8572 - val_loss: 0.4787 - val_acc: 0.8425\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4052 - acc: 0.8570 - val_loss: 0.4709 - val_acc: 0.8432\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.4014 - acc: 0.8579 - val_loss: 0.4885 - val_acc: 0.8424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4010 - acc: 0.8594 - val_loss: 0.4916 - val_acc: 0.8401\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4056 - acc: 0.8560 - val_loss: 0.4706 - val_acc: 0.8404\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4033 - acc: 0.8572 - val_loss: 0.4654 - val_acc: 0.8414\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4039 - acc: 0.8572 - val_loss: 0.4710 - val_acc: 0.8400\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4024 - acc: 0.8578 - val_loss: 0.4977 - val_acc: 0.8439\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4025 - acc: 0.8576 - val_loss: 0.4780 - val_acc: 0.8400\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4012 - acc: 0.8586 - val_loss: 0.4671 - val_acc: 0.8404\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4023 - acc: 0.8591 - val_loss: 0.4720 - val_acc: 0.8428\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.4053 - acc: 0.8578 - val_loss: 0.4734 - val_acc: 0.8413\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4039 - acc: 0.8562 - val_loss: 0.4719 - val_acc: 0.8422\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4028 - acc: 0.8571 - val_loss: 0.4904 - val_acc: 0.8384\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4022 - acc: 0.8570 - val_loss: 0.4820 - val_acc: 0.8379\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4013 - acc: 0.8585 - val_loss: 0.4888 - val_acc: 0.8364\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4020 - acc: 0.8594 - val_loss: 0.4739 - val_acc: 0.8429\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.4031 - acc: 0.8579 - val_loss: 0.4815 - val_acc: 0.8420\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3984 - acc: 0.8589 - val_loss: 0.5068 - val_acc: 0.8390\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4017 - acc: 0.8577 - val_loss: 0.4869 - val_acc: 0.8390\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4003 - acc: 0.8583 - val_loss: 0.4941 - val_acc: 0.8372\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4018 - acc: 0.8589 - val_loss: 0.4911 - val_acc: 0.8408\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4000 - acc: 0.8610 - val_loss: 0.4945 - val_acc: 0.8406\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.3974 - acc: 0.8584 - val_loss: 0.4899 - val_acc: 0.8414\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4017 - acc: 0.8578 - val_loss: 0.4849 - val_acc: 0.8399\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.4002 - acc: 0.8592 - val_loss: 0.4991 - val_acc: 0.8396\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4017 - acc: 0.8574 - val_loss: 0.4818 - val_acc: 0.8412\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3996 - acc: 0.8585 - val_loss: 0.5026 - val_acc: 0.8409\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.4010 - acc: 0.8586 - val_loss: 0.4928 - val_acc: 0.8372\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4017 - acc: 0.8580 - val_loss: 0.4761 - val_acc: 0.8427\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4005 - acc: 0.8561 - val_loss: 0.4791 - val_acc: 0.8420\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.4016 - acc: 0.8585 - val_loss: 0.5003 - val_acc: 0.8368\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4000 - acc: 0.8572 - val_loss: 0.5024 - val_acc: 0.8391\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4003 - acc: 0.8597 - val_loss: 0.4705 - val_acc: 0.8422\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3989 - acc: 0.8580 - val_loss: 0.5593 - val_acc: 0.8415\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.4000 - acc: 0.8586 - val_loss: 0.4857 - val_acc: 0.8355\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4015 - acc: 0.8581 - val_loss: 0.4801 - val_acc: 0.8408\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3990 - acc: 0.8589 - val_loss: 0.5130 - val_acc: 0.8369\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4002 - acc: 0.8586 - val_loss: 0.4925 - val_acc: 0.8411\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3984 - acc: 0.8594 - val_loss: 0.4848 - val_acc: 0.8432\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4014 - acc: 0.8591 - val_loss: 0.4668 - val_acc: 0.8393\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.4008 - acc: 0.8573 - val_loss: 0.4801 - val_acc: 0.8431\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4005 - acc: 0.8581 - val_loss: 0.4804 - val_acc: 0.8390\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3982 - acc: 0.8583 - val_loss: 0.4865 - val_acc: 0.8410\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.3994 - acc: 0.8585 - val_loss: 0.4750 - val_acc: 0.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7cf59fd4e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY2, \n",
    "          validation_data=(testX, testY2), \n",
    "          epochs=100,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9CSqKvpOIVk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGAad54JOIVm"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQ7oIymROIVp"
   },
   "outputs": [],
   "source": [
    "#First Dense Layer\n",
    "model.add(tf.keras.layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-O-fFxnOIVt"
   },
   "outputs": [],
   "source": [
    "#Add 1st hidden layer\n",
    "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiP7IL52OIVw"
   },
   "outputs": [],
   "source": [
    "#Add 2nd hidden layer\n",
    "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add OUTPUT layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr2YsZV0OIV0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4ojW6-oOIV2"
   },
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 23,196\n",
      "Trainable params: 21,628\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfFGmbZLOIV5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIkbMEN5OIV7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 1.7060 - acc: 0.4579 - val_loss: 1.1423 - val_acc: 0.6297\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.9574 - acc: 0.6818 - val_loss: 0.7920 - val_acc: 0.7296\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.7346 - acc: 0.7366 - val_loss: 0.6639 - val_acc: 0.7545\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.6401 - acc: 0.7692 - val_loss: 0.5944 - val_acc: 0.7822\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.5819 - acc: 0.7947 - val_loss: 0.5512 - val_acc: 0.8031\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5449 - acc: 0.8093 - val_loss: 0.5255 - val_acc: 0.8139\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 9s 143us/sample - loss: 0.5198 - acc: 0.8175 - val_loss: 0.5060 - val_acc: 0.8195\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.5025 - acc: 0.8229 - val_loss: 0.4936 - val_acc: 0.8234\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4892 - acc: 0.8268 - val_loss: 0.4830 - val_acc: 0.8261\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.4771 - acc: 0.8305 - val_loss: 0.4747 - val_acc: 0.8294\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4682 - acc: 0.8349 - val_loss: 0.4680 - val_acc: 0.8312\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4624 - acc: 0.8347 - val_loss: 0.4637 - val_acc: 0.8314\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4562 - acc: 0.8384 - val_loss: 0.4615 - val_acc: 0.8326\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4484 - acc: 0.8410 - val_loss: 0.4545 - val_acc: 0.8365\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.4444 - acc: 0.8422 - val_loss: 0.4507 - val_acc: 0.8380\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4380 - acc: 0.8440 - val_loss: 0.4484 - val_acc: 0.8377\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.4381 - acc: 0.8447 - val_loss: 0.4442 - val_acc: 0.8419\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.4327 - acc: 0.8471 - val_loss: 0.4429 - val_acc: 0.8404\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4274 - acc: 0.8485 - val_loss: 0.4381 - val_acc: 0.8428\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.4235 - acc: 0.8497 - val_loss: 0.4370 - val_acc: 0.8430\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4204 - acc: 0.8500 - val_loss: 0.4331 - val_acc: 0.8433\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 9s 145us/sample - loss: 0.4151 - acc: 0.8523 - val_loss: 0.4298 - val_acc: 0.8441\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4143 - acc: 0.8519 - val_loss: 0.4298 - val_acc: 0.8432\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4131 - acc: 0.8520 - val_loss: 0.4231 - val_acc: 0.8470\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 0.4060 - acc: 0.8555 - val_loss: 0.4218 - val_acc: 0.8474\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4072 - acc: 0.8549 - val_loss: 0.4234 - val_acc: 0.8474\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4041 - acc: 0.8558 - val_loss: 0.4208 - val_acc: 0.8478\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4019 - acc: 0.8555 - val_loss: 0.4192 - val_acc: 0.8485\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3999 - acc: 0.8573 - val_loss: 0.4180 - val_acc: 0.8491\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 8s 137us/sample - loss: 0.3963 - acc: 0.8595 - val_loss: 0.4145 - val_acc: 0.8507\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3948 - acc: 0.8586 - val_loss: 0.4126 - val_acc: 0.8491\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.3932 - acc: 0.8596 - val_loss: 0.4090 - val_acc: 0.8510\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.3911 - acc: 0.8608 - val_loss: 0.4096 - val_acc: 0.8530\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.3878 - acc: 0.8615 - val_loss: 0.4088 - val_acc: 0.8542\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.3862 - acc: 0.8613 - val_loss: 0.4043 - val_acc: 0.8530\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.3843 - acc: 0.8633 - val_loss: 0.4092 - val_acc: 0.8530\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.3819 - acc: 0.8624 - val_loss: 0.4040 - val_acc: 0.8524\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.3826 - acc: 0.8630 - val_loss: 0.4028 - val_acc: 0.8538\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.3799 - acc: 0.8641 - val_loss: 0.4003 - val_acc: 0.8564\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.3782 - acc: 0.8645 - val_loss: 0.3969 - val_acc: 0.8587\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.3742 - acc: 0.8661 - val_loss: 0.3988 - val_acc: 0.8593\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3734 - acc: 0.8663 - val_loss: 0.3995 - val_acc: 0.8556\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 9s 142us/sample - loss: 0.3751 - acc: 0.8656 - val_loss: 0.3969 - val_acc: 0.8587\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3734 - acc: 0.8655 - val_loss: 0.3969 - val_acc: 0.8581\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.3706 - acc: 0.8666 - val_loss: 0.3954 - val_acc: 0.8570\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.3696 - acc: 0.8676 - val_loss: 0.3948 - val_acc: 0.8582\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.3690 - acc: 0.8672 - val_loss: 0.3972 - val_acc: 0.8576\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.3669 - acc: 0.8687 - val_loss: 0.3913 - val_acc: 0.8569\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3657 - acc: 0.8682 - val_loss: 0.3900 - val_acc: 0.8583\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.3650 - acc: 0.8687 - val_loss: 0.3900 - val_acc: 0.8613\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.3651 - acc: 0.8683 - val_loss: 0.3937 - val_acc: 0.8568\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.3609 - acc: 0.8703 - val_loss: 0.3926 - val_acc: 0.8601\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.3618 - acc: 0.8708 - val_loss: 0.3906 - val_acc: 0.8607\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3603 - acc: 0.8710 - val_loss: 0.3961 - val_acc: 0.8570\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.3591 - acc: 0.8706 - val_loss: 0.3887 - val_acc: 0.8596\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.3603 - acc: 0.8707 - val_loss: 0.3894 - val_acc: 0.8592\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.3586 - acc: 0.8717 - val_loss: 0.3895 - val_acc: 0.8582\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3563 - acc: 0.8718 - val_loss: 0.3864 - val_acc: 0.8599\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.3571 - acc: 0.8715 - val_loss: 0.3874 - val_acc: 0.8617\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.3546 - acc: 0.8726 - val_loss: 0.3856 - val_acc: 0.8595\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.3533 - acc: 0.8731 - val_loss: 0.3854 - val_acc: 0.8612\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.3530 - acc: 0.8734 - val_loss: 0.3850 - val_acc: 0.8598\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 0.3505 - acc: 0.8735 - val_loss: 0.3874 - val_acc: 0.8616\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.3506 - acc: 0.8732 - val_loss: 0.3857 - val_acc: 0.8616\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3492 - acc: 0.8743 - val_loss: 0.3887 - val_acc: 0.8584\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.3481 - acc: 0.8750 - val_loss: 0.3874 - val_acc: 0.8609\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3494 - acc: 0.8748 - val_loss: 0.3843 - val_acc: 0.8615\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.3451 - acc: 0.8756 - val_loss: 0.3848 - val_acc: 0.8604\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.3472 - acc: 0.8748 - val_loss: 0.3866 - val_acc: 0.8606\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3469 - acc: 0.8749 - val_loss: 0.3800 - val_acc: 0.8636\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.3466 - acc: 0.8749 - val_loss: 0.3848 - val_acc: 0.8612\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3448 - acc: 0.8753 - val_loss: 0.3834 - val_acc: 0.8617\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3454 - acc: 0.8755 - val_loss: 0.3821 - val_acc: 0.8609\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.3449 - acc: 0.8755 - val_loss: 0.3841 - val_acc: 0.8636\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3430 - acc: 0.8759 - val_loss: 0.3802 - val_acc: 0.8629\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.3422 - acc: 0.8767 - val_loss: 0.3821 - val_acc: 0.8614\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3416 - acc: 0.8780 - val_loss: 0.3804 - val_acc: 0.8621\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.3403 - acc: 0.8767 - val_loss: 0.3819 - val_acc: 0.8628\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.3395 - acc: 0.8778 - val_loss: 0.3828 - val_acc: 0.8619\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3391 - acc: 0.8770 - val_loss: 0.3814 - val_acc: 0.8612\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.3362 - acc: 0.8792 - val_loss: 0.3835 - val_acc: 0.8620\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.3373 - acc: 0.8775 - val_loss: 0.3802 - val_acc: 0.8616\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.3378 - acc: 0.8780 - val_loss: 0.3821 - val_acc: 0.8630\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.3365 - acc: 0.8782 - val_loss: 0.3816 - val_acc: 0.8620\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 9s 147us/sample - loss: 0.3372 - acc: 0.8774 - val_loss: 0.3814 - val_acc: 0.8630\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3361 - acc: 0.8781 - val_loss: 0.3855 - val_acc: 0.8612\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.3369 - acc: 0.8777 - val_loss: 0.3816 - val_acc: 0.8631\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.3325 - acc: 0.8793 - val_loss: 0.3829 - val_acc: 0.8617\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3337 - acc: 0.8791 - val_loss: 0.3804 - val_acc: 0.8634\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.3326 - acc: 0.8805 - val_loss: 0.3797 - val_acc: 0.8637\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.3336 - acc: 0.8789 - val_loss: 0.3827 - val_acc: 0.8624\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.3324 - acc: 0.8804 - val_loss: 0.3810 - val_acc: 0.8628\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.3306 - acc: 0.8808 - val_loss: 0.3823 - val_acc: 0.8626\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.3294 - acc: 0.8809 - val_loss: 0.3826 - val_acc: 0.8611\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.3298 - acc: 0.8811 - val_loss: 0.3797 - val_acc: 0.8638\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.3326 - acc: 0.8792 - val_loss: 0.3822 - val_acc: 0.8618\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3298 - acc: 0.8806 - val_loss: 0.3786 - val_acc: 0.8621\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.3306 - acc: 0.8808 - val_loss: 0.3790 - val_acc: 0.8636\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.3281 - acc: 0.8812 - val_loss: 0.3819 - val_acc: 0.8624\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.3273 - acc: 0.8815 - val_loss: 0.3803 - val_acc: 0.8628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7cff8b6e80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY2, \n",
    "          validation_data=(testX, testY2), \n",
    "          epochs=100,\n",
    "          batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Classification_F-MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
